{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation from scratch of a Vision Transformer**"
      ],
      "metadata": {
        "id": "KXN4WJurhqtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I) Model**"
      ],
      "metadata": {
        "id": "LvU21jPOhzzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "CNoZ6Q6VF82F"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "57BiKRJ8lWCE"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Patch**"
      ],
      "metadata": {
        "id": "23tl5nt7F3_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patcher(nn.Module):\n",
        "    def __init__(self, patch_length):\n",
        "        super().__init__()\n",
        "        self.patch_length = patch_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "        il faut revoir la manière dont on fait le patch, ça ne le fait pas de la bonne manière\n",
        "        \"\"\"\n",
        "        batch_size, h, w = x.shape\n",
        "        nb_patchs = h*w // (self.patch_length**2)\n",
        "\n",
        "        res = torch.ones(batch_size, nb_patchs, self.patch_length**2)\n",
        "\n",
        "        for batch in range(batch_size):\n",
        "          for i in range(0, h, self.patch_length):\n",
        "            for j in range(0, w, self.patch_length):\n",
        "              patch_index = ( (h // self.patch_length) * i + j ) // self.patch_length\n",
        "              res[batch, patch_index] = torch.flatten(x[batch, i:i+self.patch_length, j:j+self.patch_length])\n",
        "\n",
        "        cls = torch.ones(batch_size, 1, self.patch_length**2)\n",
        "        res = torch.cat((cls, res), 1)\n",
        "\n",
        "        return res\n",
        "\n",
        "patcher = Patcher(patch_length=2)\n",
        "x = torch.rand(32, 28, 28)\n",
        "res = patcher(x)\n",
        "print(res.shape)\n",
        "assert torch.equal(res[0, 1], torch.flatten(x[0, 0:2, 0:2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61OsIgExJSOD",
        "outputId": "7c77c571-eebe-43a2-8e18-a01e482b89ba"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 197, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Encoder**"
      ],
      "metadata": {
        "id": "hWI3prq_F6KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Encoder part, I re-used a code I had already written to implement a transformer from scratch (at https://github.com/yanisadel/ML-from-scratch, file Transformer.ipynb)"
      ],
      "metadata": {
        "id": "88SMGa2khF5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_embedding(batch_size, nb_tokens, embedding_dim):\n",
        "  \"\"\"\n",
        "  Takes as input the shape (batch_size, nb_tokens, embedding_dim),\n",
        "  and returns a positional encoding tensor with shape (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  embedding = torch.rand(nb_tokens, embedding_dim)\n",
        "  for pos in range(nb_tokens):\n",
        "    for i in range(0, embedding_dim, 2):\n",
        "      embedding[pos][i] = torch.sin(torch.Tensor([pos / 10000**(2*i/embedding_dim)]))\n",
        "    for i in range(1, embedding_dim, 2):\n",
        "      embedding[pos][i] = torch.cos(torch.Tensor([pos / 10000**(2*i/embedding_dim)]))\n",
        "\n",
        "  return embedding.repeat(batch_size, 1, 1)\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a query, a key and a value,\n",
        "  that have shape (batch_size, nb_tokens, embedding_dim),\n",
        "  performs Scaled-Dot-Product Attention \n",
        "  and returns a (batch_size, nb_tokens, embedding_dim) tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.query_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None, training=False):\n",
        "    batch_size, nb_tokens, embedding_dim = query.shape\n",
        "    Q = self.query_layer(query)\n",
        "    K = self.key_layer(key)\n",
        "    V = self.value_layer(value)\n",
        "    K_transpose = torch.transpose(K, 1, 2)\n",
        "    QK = torch.matmul(Q, K_transpose)\n",
        "    \n",
        "    QK_normalized = QK / (embedding_dim**(1/2))\n",
        "\n",
        "    if mask != None:\n",
        "      QK_shape = QK_normalized.shape\n",
        "      if len(mask.shape) == len(QK_shape) - 1:\n",
        "        mask = mask.unsqueeze(1).repeat(1, mask.shape[-1], 1)\n",
        "      \n",
        "      if mask.shape != QK_shape:\n",
        "        raise Exception(\"The shape of the mask is not correct (the shape is {} instead of {} or {})\".format(mask.shape, QK_shape[:-1], QK_shape))\n",
        "\n",
        "      QK_normalized *= mask\n",
        "              \n",
        "    softmax = nn.Softmax(dim=2)(QK_normalized)\n",
        "    res = torch.matmul(softmax, V)\n",
        "\n",
        "    return res\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a query, a key and a value,\n",
        "  that have shape (batch_size, nb_tokens, embedding_dim),\n",
        "  performs a Multi-head Attention over the token embeddings \n",
        "  and returns a (batch_size, nb_tokens, embedding_dim) tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, attention_heads, embedding_dim, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.attention_heads = attention_heads\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.attention_layers = [ScaledDotProductAttention(embedding_dim=embedding_dim//attention_heads) for _ in range(attention_heads)]\n",
        "\n",
        "  def forward(self, query, key, value, mask=None, training=False):\n",
        "    batch_size, nb_tokens, embedding_dim = query.shape\n",
        "    query = query.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    key = key.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    value = value.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    \n",
        "    concat = torch.Tensor()\n",
        "    for i, attention_layer in enumerate(self.attention_layers):\n",
        "      attention = attention_layer(query[:, :, i, :], key[:, :, i, :], value[:, :, i, :], mask=mask, training=training)\n",
        "      concat = torch.concat([concat, attention], dim=2)\n",
        "  \n",
        "    return concat\n",
        "\n",
        "class FeedforwardLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a tensor x (batch_size, nb_tokens, embedding_dim),\n",
        "  and passes it through a dense, a relu, a dense, and a dropout layer\n",
        "  It returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.dense1 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.dense2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, training=False):\n",
        "    x = self.dense1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dense2(x)\n",
        "    if training:\n",
        "      x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  EncoderLayer that takes as input a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  and returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  It represents one stack of the Encoder, so it performs multi head attention,\n",
        "  then layer normalization, and then feedforward network\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, attention_heads, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.multi_head_attention = MultiHeadAttentionLayer(attention_heads=8, embedding_dim=embedding_dim)\n",
        "    self.feedforward = FeedforwardLayer(embedding_dim=embedding_dim, dropout_rate=dropout_rate)\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, mask=None, training=False):\n",
        "    mha_output = self.multi_head_attention(x, x, x, mask=mask, training=training)\n",
        "    if training:\n",
        "      multi_head_attention_output = self.dropout(mha_output)\n",
        "    x = self.layer_norm(x + mha_output)\n",
        "    x = self.layer_norm(x + self.feedforward(x, training=training))\n",
        "\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \"\"\"\n",
        "  Encoder of the transformer that takes as input a tensor (batch_size, nb_tokens)\n",
        "  and returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.embedding_layer = nn.Linear(patch_length**2, embedding_dim)\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim=embedding_dim, attention_heads=attention_heads, dropout_rate=dropout_rate) for _ in range(nb_layers)])\n",
        "\n",
        "  def forward(self, x, mask=None, training=False):\n",
        "    x = self.embedding_layer(x)\n",
        "    x += get_positional_embedding(x.shape[0], x.shape[1], x.shape[2])\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask=mask, training=training)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = Encoder(nb_layers=6, patch_length=4, embedding_dim=512, attention_heads=8, dropout_rate=0.1)\n",
        "x = torch.rand(32, 50, 16)\n",
        "res = model(x)\n",
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdW6xQHcG0sI",
        "outputId": "ac875a98-eae6-47c0-a8f0-5ef12bd11fcf"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Vision Transformer**"
      ],
      "metadata": {
        "id": "_MfGN0F9cGrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a batch of images (batch_size, height, width) (with height=width)\n",
        "  and returns a tensor (batch_size, height*width / (patch_length**2), embedding_dim)\n",
        "\n",
        "  It separates the image into patches, flattens the patches, then it goes through a transformer,\n",
        "  and through a MLP layer with a softmax layer to get class probabilities\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate):\n",
        "    \"\"\"\n",
        "      num_classes (int): number of classes we want to predict\n",
        "      nb_layers (int): number of layers of the encoder\n",
        "      patch_length (int): length of the patchs on the image\n",
        "      embedding_dim (int): embedding dimension for the vectors inside the transformer\n",
        "      attention_heads (int): number of attention heads inside the transformer\n",
        "      dropout_rate (int): dropout rate for the MLP layers inside the transformer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.patcher = Patcher(patch_length)\n",
        "    self.encoder = Encoder(nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate)\n",
        "    self.linear = nn.Linear(embedding_dim, num_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x, training=False):\n",
        "    x = self.patcher(x)\n",
        "    x = self.encoder(x, training=training)\n",
        "    x = x[:, 0] # get cls_token\n",
        "    x = self.linear(x)\n",
        "    x = self.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "x = torch.rand(32, 28, 28)\n",
        "model = ViT(num_classes=10, nb_layers=6, patch_length=4, embedding_dim=512, attention_heads=8, dropout_rate=0.1)\n",
        "assert model(x, training=True).shape == (32, 10)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_07N5UUcJ6W",
        "outputId": "2a335740-f0c2-41a8-c624-aff08de8d50e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patcher): Patcher()\n",
              "  (encoder): Encoder(\n",
              "    (embedding_layer): Linear(in_features=16, out_features=512, bias=True)\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II) Test**"
      ],
      "metadata": {
        "id": "UilcepW4iCUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Récupération des données**"
      ],
      "metadata": {
        "id": "INCjtlQ1D10A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TvLOe9j8qHU",
        "outputId": "755817bc-9bb0-4f7d-e180-1bb5596d3d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/IA/kaggle.json /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TEY5ifr580Ri"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c digit-recognizer"
      ],
      "metadata": {
        "id": "qlxWJi7K86Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip digit-recognizer.zip"
      ],
      "metadata": {
        "id": "eiZBH8DN9FSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Chargement des données et visualisation**"
      ],
      "metadata": {
        "id": "dZzyVxdRD7eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-3ODYsA99KvL"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "print(df.shape)\n",
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "BQBUugpg9NW7",
        "outputId": "47073bd2-383d-452c-f18e-f5e6ddddd326"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 785)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
              "0      1       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel780  pixel781  pixel782  pixel783  \n",
              "0         0         0         0         0  \n",
              "\n",
              "[1 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b100568-ace7-4747-a4c5-5c7bebba59f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b100568-ace7-4747-a4c5-5c7bebba59f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b100568-ace7-4747-a4c5-5c7bebba59f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b100568-ace7-4747-a4c5-5c7bebba59f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['label'].to_numpy()\n",
        "X = df.drop(columns=['label']).to_numpy().reshape(-1, 28, 28)"
      ],
      "metadata": {
        "id": "fP2VwVHL9bNb"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[3])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mbW_OQC4EDjb",
        "outputId": "c9477adf-6ee6-46c0-e6a8-30f0f8de7d9a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Dataloader**"
      ],
      "metadata": {
        "id": "J-NnRBSwijyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Data(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset qui contient deux listes de strings (text1 et text2) ainsi qu'une liste de similarités entre ces strings (target)\n",
        "    \"\"\"\n",
        "    def __init__(self, images, labels):      \n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return image, label\n",
        "  \n",
        "\n",
        "dataset = Data(X[:2000], y[:2000])\n",
        "test_size = 0.2\n",
        "nb_rows = len(dataset)\n",
        "nb_rows_train = int((1 - test_size) * nb_rows)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [nb_rows_train, nb_rows - nb_rows_train])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "3bay_L8oiSKl"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) Define model and optimizer**"
      ],
      "metadata": {
        "id": "7Sww7-bKiKpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_length = 4\n",
        "nb_layers = 4\n",
        "embedding_dim = 512\n",
        "attention_heads = 8\n",
        "dropout_rate = 0.1\n",
        "num_classes = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "print(\"Number of patchs on an image : \", 28**2 / (patch_length**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFyCJX8jYW_",
        "outputId": "d7d87a50-b653-4736-d30a-fe32aa92a92e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patchs on an image :  49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(num_classes=num_classes, \n",
        "            nb_layers=nb_layers, \n",
        "            patch_length=patch_length, \n",
        "            embedding_dim=embedding_dim, \n",
        "            attention_heads=attention_heads, \n",
        "            dropout_rate=dropout_rate)\n",
        "loss_object = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "wCjap_kSjHEN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) Train**"
      ],
      "metadata": {
        "id": "71WoPUaCj-OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss_train = 0\n",
        "    running_loss_test = 0\n",
        "    print(\"Epoch {}/{}\".format(epoch+1, EPOCHS))\n",
        "    for images, labels in tqdm(train_dataloader):\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(images, training=True)\n",
        "        loss = loss_object(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "    running_loss_train /= len(train_dataloader)\n",
        "    losses_train.append(running_loss_train)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "          outputs = model(images)\n",
        "          loss = loss_object(outputs, labels)\n",
        "          running_loss_test += loss.item()\n",
        "\n",
        "    running_loss_test /= len(test_dataloader)\n",
        "    losses_test.append(running_loss_test)\n",
        "\n",
        "    print(\"Train Loss : {}, Test Loss : {}\".format(running_loss_train, running_loss_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZtzArq7j_7d",
        "outputId": "c15b5202-5a8e-4755-c989-370a3d226d7d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.285724139213562, Test Loss : 2.2894969903505764\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:22<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.253756012916565, Test Loss : 2.279659179540781\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:20<00:00,  1.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.227060856819153, Test Loss : 2.234089759679941\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.285259804725647, Test Loss : 2.374514543093168\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [01:18<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.319807324409485, Test Loss : 2.3216814261216383\n"
          ]
        }
      ]
    }
  ]
}