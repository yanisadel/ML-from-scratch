{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation from scratch of a Vision Transformer**"
      ],
      "metadata": {
        "id": "KXN4WJurhqtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I) Model**"
      ],
      "metadata": {
        "id": "LvU21jPOhzzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "CNoZ6Q6VF82F"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "57BiKRJ8lWCE"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Patch**"
      ],
      "metadata": {
        "id": "23tl5nt7F3_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patcher(nn.Module):\n",
        "    def __init__(self, patch_length):\n",
        "        super().__init__()\n",
        "        self.patch_length = patch_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "        il faut revoir la manière dont on fait le patch, ça ne le fait pas de la bonne manière\n",
        "        \"\"\"\n",
        "        batch_size, h, w = x.shape\n",
        "        nb_patchs = h*w // (self.patch_length**2)\n",
        "        x = x.reshape(batch_size, h*w // (self.patch_length**2), self.patch_length**2)\n",
        "        cls = torch.ones(batch_size, 1, self.patch_length**2).to(device)\n",
        "        x = torch.cat((cls, x), 1)\n",
        "        return x\n",
        "\n",
        "patcher = Patcher(patch_length=2)\n",
        "x = torch.rand(32, 28, 28).to(device)\n",
        "print(x[0, 0:4, 0:4])\n",
        "print(patcher(x)[0, 1])\n",
        "patcher(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61OsIgExJSOD",
        "outputId": "d48330e7-d46d-4402-c927-a6584679a7d8"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3938, 0.5746, 0.4134, 0.6659],\n",
            "        [0.5102, 0.7362, 0.5131, 0.3412],\n",
            "        [0.9540, 0.0383, 0.8737, 0.0038],\n",
            "        [0.1547, 0.3071, 0.0420, 0.9894]])\n",
            "tensor([0.3938, 0.5746, 0.4134, 0.6659])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor([i for i in range(1, 10*10+1)])\n",
        "x = x.reshape(10, 10)\n",
        "print(x.shape)\n",
        "x\n",
        "# we want x to become [25, 4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5rjQIHjls6c",
        "outputId": "b296441f-977e-4704-b895-bd54d09c2f86"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
              "        [ 11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.],\n",
              "        [ 21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.],\n",
              "        [ 31.,  32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.],\n",
              "        [ 41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.],\n",
              "        [ 51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.],\n",
              "        [ 61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.],\n",
              "        [ 71.,  72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.],\n",
              "        [ 81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.],\n",
              "        [ 91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(5, 2, 5, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSUY_K7imbAQ",
        "outputId": "975275ed-4421-419c-c460-8eefa79a8def"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[  1.,   2.],\n",
              "          [  3.,   4.],\n",
              "          [  5.,   6.],\n",
              "          [  7.,   8.],\n",
              "          [  9.,  10.]],\n",
              "\n",
              "         [[ 11.,  12.],\n",
              "          [ 13.,  14.],\n",
              "          [ 15.,  16.],\n",
              "          [ 17.,  18.],\n",
              "          [ 19.,  20.]]],\n",
              "\n",
              "\n",
              "        [[[ 21.,  22.],\n",
              "          [ 23.,  24.],\n",
              "          [ 25.,  26.],\n",
              "          [ 27.,  28.],\n",
              "          [ 29.,  30.]],\n",
              "\n",
              "         [[ 31.,  32.],\n",
              "          [ 33.,  34.],\n",
              "          [ 35.,  36.],\n",
              "          [ 37.,  38.],\n",
              "          [ 39.,  40.]]],\n",
              "\n",
              "\n",
              "        [[[ 41.,  42.],\n",
              "          [ 43.,  44.],\n",
              "          [ 45.,  46.],\n",
              "          [ 47.,  48.],\n",
              "          [ 49.,  50.]],\n",
              "\n",
              "         [[ 51.,  52.],\n",
              "          [ 53.,  54.],\n",
              "          [ 55.,  56.],\n",
              "          [ 57.,  58.],\n",
              "          [ 59.,  60.]]],\n",
              "\n",
              "\n",
              "        [[[ 61.,  62.],\n",
              "          [ 63.,  64.],\n",
              "          [ 65.,  66.],\n",
              "          [ 67.,  68.],\n",
              "          [ 69.,  70.]],\n",
              "\n",
              "         [[ 71.,  72.],\n",
              "          [ 73.,  74.],\n",
              "          [ 75.,  76.],\n",
              "          [ 77.,  78.],\n",
              "          [ 79.,  80.]]],\n",
              "\n",
              "\n",
              "        [[[ 81.,  82.],\n",
              "          [ 83.,  84.],\n",
              "          [ 85.,  86.],\n",
              "          [ 87.,  88.],\n",
              "          [ 89.,  90.]],\n",
              "\n",
              "         [[ 91.,  92.],\n",
              "          [ 93.,  94.],\n",
              "          [ 95.,  96.],\n",
              "          [ 97.,  98.],\n",
              "          [ 99., 100.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_length = 2\n",
        "h, w = x.shape\n",
        "nb_patchs = h*w // (patch_length**2)\n",
        "a = x.reshape(h*w // (patch_length**2), patch_length**2)\n",
        "print(a.shape)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnbgkP2fl7sU",
        "outputId": "9cbbce5a-b424-48b2-fc50-ddf16995de3e"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1.,   2.,   3.,   4.],\n",
              "        [  5.,   6.,   7.,   8.],\n",
              "        [  9.,  10.,  11.,  12.],\n",
              "        [ 13.,  14.,  15.,  16.],\n",
              "        [ 17.,  18.,  19.,  20.],\n",
              "        [ 21.,  22.,  23.,  24.],\n",
              "        [ 25.,  26.,  27.,  28.],\n",
              "        [ 29.,  30.,  31.,  32.],\n",
              "        [ 33.,  34.,  35.,  36.],\n",
              "        [ 37.,  38.,  39.,  40.],\n",
              "        [ 41.,  42.,  43.,  44.],\n",
              "        [ 45.,  46.,  47.,  48.],\n",
              "        [ 49.,  50.,  51.,  52.],\n",
              "        [ 53.,  54.,  55.,  56.],\n",
              "        [ 57.,  58.,  59.,  60.],\n",
              "        [ 61.,  62.,  63.,  64.],\n",
              "        [ 65.,  66.,  67.,  68.],\n",
              "        [ 69.,  70.,  71.,  72.],\n",
              "        [ 73.,  74.,  75.,  76.],\n",
              "        [ 77.,  78.,  79.,  80.],\n",
              "        [ 81.,  82.,  83.,  84.],\n",
              "        [ 85.,  86.,  87.,  88.],\n",
              "        [ 89.,  90.,  91.,  92.],\n",
              "        [ 93.,  94.,  95.,  96.],\n",
              "        [ 97.,  98.,  99., 100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(patcher(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QICKqkwifB1D",
        "outputId": "ded34235-eb59-4b90-844b-e5dcb1a3e1ea"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4675, 0.8974, 0.0564, 0.3351],\n",
            "         [0.8424, 0.3917, 0.0342, 0.0985],\n",
            "         [0.7824, 0.5111, 0.1314, 0.8606],\n",
            "         ...,\n",
            "         [0.2343, 0.6827, 0.1314, 0.8409],\n",
            "         [0.4755, 0.6047, 0.4218, 0.9799],\n",
            "         [0.4734, 0.6247, 0.5995, 0.1768]],\n",
            "\n",
            "        [[0.1216, 0.4021, 0.0122, 0.5377],\n",
            "         [0.2581, 0.9028, 0.9501, 0.1691],\n",
            "         [0.9899, 0.2495, 0.7475, 0.0566],\n",
            "         ...,\n",
            "         [0.3826, 0.3667, 0.0707, 0.4728],\n",
            "         [0.5153, 0.6449, 0.9547, 0.7285],\n",
            "         [0.3030, 0.6299, 0.9417, 0.1074]],\n",
            "\n",
            "        [[0.7892, 0.4406, 0.3668, 0.0736],\n",
            "         [0.3629, 0.0676, 0.3202, 0.6747],\n",
            "         [0.6208, 0.8712, 0.2655, 0.3876],\n",
            "         ...,\n",
            "         [0.7761, 0.1230, 0.0064, 0.9200],\n",
            "         [0.6132, 0.1640, 0.3516, 0.9999],\n",
            "         [0.5869, 0.8238, 0.9659, 0.9412]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.1208, 0.6994, 0.4493, 0.8310],\n",
            "         [0.6039, 0.2233, 0.3540, 0.8728],\n",
            "         [0.9262, 0.9724, 0.8310, 0.2994],\n",
            "         ...,\n",
            "         [0.0017, 0.1480, 0.5512, 0.4080],\n",
            "         [0.8026, 0.5977, 0.1323, 0.5681],\n",
            "         [0.1792, 0.9035, 0.9308, 0.4794]],\n",
            "\n",
            "        [[0.0029, 0.5219, 0.2803, 0.1659],\n",
            "         [0.1797, 0.3531, 0.4604, 0.1393],\n",
            "         [0.3248, 0.0131, 0.4135, 0.7130],\n",
            "         ...,\n",
            "         [0.8421, 0.2701, 0.2706, 0.3247],\n",
            "         [0.1652, 0.0817, 0.5532, 0.1331],\n",
            "         [0.4603, 0.5609, 0.4447, 0.1653]],\n",
            "\n",
            "        [[0.9585, 0.3626, 0.2533, 0.2782],\n",
            "         [0.6768, 0.1108, 0.7187, 0.8415],\n",
            "         [0.1271, 0.3446, 0.5629, 0.2354],\n",
            "         ...,\n",
            "         [0.2571, 0.7460, 0.6754, 0.4488],\n",
            "         [0.6236, 0.2519, 0.7161, 0.9633],\n",
            "         [0.2264, 0.1717, 0.3877, 0.0971]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Encoder**"
      ],
      "metadata": {
        "id": "hWI3prq_F6KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Encoder part, I re-used a code I had already written to implement a transformer from scratch (at https://github.com/yanisadel/ML-from-scratch, file Transformer.ipynb)"
      ],
      "metadata": {
        "id": "88SMGa2khF5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_embedding(batch_size, nb_tokens, embedding_dim):\n",
        "  \"\"\"\n",
        "  Takes as input the shape (batch_size, nb_tokens, embedding_dim),\n",
        "  and returns a positional encoding tensor with shape (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  embedding = torch.rand(nb_tokens, embedding_dim)\n",
        "  for pos in range(nb_tokens):\n",
        "    for i in range(0, embedding_dim, 2):\n",
        "      embedding[pos][i] = torch.sin(torch.Tensor([pos / 10000**(2*i/embedding_dim)]))\n",
        "    for i in range(1, embedding_dim, 2):\n",
        "      embedding[pos][i] = torch.cos(torch.Tensor([pos / 10000**(2*i/embedding_dim)]))\n",
        "\n",
        "  return embedding.repeat(batch_size, 1, 1).to(device)\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a query, a key and a value,\n",
        "  that have shape (batch_size, nb_tokens, embedding_dim),\n",
        "  performs Scaled-Dot-Product Attention \n",
        "  and returns a (batch_size, nb_tokens, embedding_dim) tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.query_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value_layer = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None, training=False):\n",
        "    batch_size, nb_tokens, embedding_dim = query.shape\n",
        "    Q = self.query_layer(query)\n",
        "    K = self.key_layer(key)\n",
        "    V = self.value_layer(value)\n",
        "    K_transpose = torch.transpose(K, 1, 2)\n",
        "    QK = torch.matmul(Q, K_transpose)\n",
        "    \n",
        "    QK_normalized = QK / (embedding_dim**(1/2))\n",
        "\n",
        "    if mask != None:\n",
        "      QK_shape = QK_normalized.shape\n",
        "      if len(mask.shape) == len(QK_shape) - 1:\n",
        "        mask = mask.unsqueeze(1).repeat(1, mask.shape[-1], 1)\n",
        "      \n",
        "      if mask.shape != QK_shape:\n",
        "        raise Exception(\"The shape of the mask is not correct (the shape is {} instead of {} or {})\".format(mask.shape, QK_shape[:-1], QK_shape))\n",
        "\n",
        "      QK_normalized *= mask\n",
        "              \n",
        "    softmax = nn.Softmax(dim=2)(QK_normalized)\n",
        "    res = torch.matmul(softmax, V)\n",
        "\n",
        "    return res\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a query, a key and a value,\n",
        "  that have shape (batch_size, nb_tokens, embedding_dim),\n",
        "  performs a Multi-head Attention over the token embeddings \n",
        "  and returns a (batch_size, nb_tokens, embedding_dim) tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, attention_heads, embedding_dim, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.attention_heads = attention_heads\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.attention_layers = [ScaledDotProductAttention(embedding_dim=embedding_dim//attention_heads) for _ in range(attention_heads)]\n",
        "\n",
        "  def forward(self, query, key, value, mask=None, training=False):\n",
        "    batch_size, nb_tokens, embedding_dim = query.shape\n",
        "    query = query.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    key = key.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    value = value.reshape(batch_size, nb_tokens, self.attention_heads, embedding_dim // self.attention_heads)\n",
        "    \n",
        "    concat = torch.Tensor()\n",
        "    for i, attention_layer in enumerate(self.attention_layers):\n",
        "      attention = attention_layer(query[:, :, i, :], key[:, :, i, :], value[:, :, i, :], mask=mask, training=training)\n",
        "      concat = torch.concat([concat, attention], dim=2)\n",
        "  \n",
        "    return concat\n",
        "\n",
        "class FeedforwardLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a tensor x (batch_size, nb_tokens, embedding_dim),\n",
        "  and passes it through a dense, a relu, a dense, and a dropout layer\n",
        "  It returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.dense1 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.dense2 = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, training=False):\n",
        "    x = self.dense1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dense2(x)\n",
        "    if training:\n",
        "      x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  EncoderLayer that takes as input a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  and returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  It represents one stack of the Encoder, so it performs multi head attention,\n",
        "  then layer normalization, and then feedforward network\n",
        "  \"\"\"\n",
        "  def __init__(self, embedding_dim, attention_heads, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.multi_head_attention = MultiHeadAttentionLayer(attention_heads=8, embedding_dim=embedding_dim)\n",
        "    self.feedforward = FeedforwardLayer(embedding_dim=embedding_dim, dropout_rate=dropout_rate)\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, mask=None, training=False):\n",
        "    mha_output = self.multi_head_attention(x, x, x, mask=mask, training=training)\n",
        "    if training:\n",
        "      multi_head_attention_output = self.dropout(mha_output)\n",
        "    x = self.layer_norm(x + mha_output)\n",
        "    x = self.layer_norm(x + self.feedforward(x, training=training))\n",
        "\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \"\"\"\n",
        "  Encoder of the transformer that takes as input a tensor (batch_size, nb_tokens)\n",
        "  and returns a tensor (batch_size, nb_tokens, embedding_dim)\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate, **kwargs):\n",
        "    super(**kwargs).__init__()\n",
        "    self.embedding_layer = nn.Linear(patch_length**2, embedding_dim)\n",
        "    self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim=embedding_dim, attention_heads=attention_heads, dropout_rate=dropout_rate) for _ in range(nb_layers)])\n",
        "\n",
        "  def forward(self, x, mask=None, training=False):\n",
        "    x = self.embedding_layer(x)\n",
        "    x += get_positional_embedding(x.shape[0], x.shape[1], x.shape[2])\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask=mask, training=training)\n",
        "\n",
        "    return x\n",
        "\n",
        "model = Encoder(nb_layers=6, patch_length=4, embedding_dim=512, attention_heads=8, dropout_rate=0.1).to(device)\n",
        "x = torch.rand(32, 50, 16).to(device)\n",
        "res = model(x)\n",
        "res.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdW6xQHcG0sI",
        "outputId": "ac6cbb25-add2-4073-81c3-64fe645edcdf"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Vision Transformer**"
      ],
      "metadata": {
        "id": "_MfGN0F9cGrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes as input a batch of images (batch_size, height, width) (with height=width)\n",
        "  and returns a tensor (batch_size, height*width / (patch_length**2), embedding_dim)\n",
        "\n",
        "  It separates the image into patches, flattens the patches, then it goes through a transformer,\n",
        "  and through a MLP layer with a softmax layer to get class probabilities\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate):\n",
        "    \"\"\"\n",
        "      num_classes (int): number of classes we want to predict\n",
        "      nb_layers (int): number of layers of the encoder\n",
        "      patch_length (int): length of the patchs on the image\n",
        "      embedding_dim (int): embedding dimension for the vectors inside the transformer\n",
        "      attention_heads (int): number of attention heads inside the transformer\n",
        "      dropout_rate (int): dropout rate for the MLP layers inside the transformer\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.patcher = Patcher(patch_length)\n",
        "    self.encoder = Encoder(nb_layers, patch_length, embedding_dim, attention_heads, dropout_rate)\n",
        "    self.linear = nn.Linear(embedding_dim, num_classes)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x, training=False):\n",
        "    x = self.patcher(x)\n",
        "    x = self.encoder(x, training=training)\n",
        "    x = x[:, 0] # get cls_token\n",
        "    x = self.linear(x)\n",
        "    x = self.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "x = torch.rand(32, 28, 28).to(device)\n",
        "model = ViT(num_classes=10, nb_layers=6, patch_length=4, embedding_dim=512, attention_heads=8, dropout_rate=0.1).to(device)\n",
        "model(x, training=True).shape\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_07N5UUcJ6W",
        "outputId": "b468aee3-3cc9-4a44-dcda-4a2bc28c8498"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patcher): Patcher()\n",
              "  (encoder): Encoder(\n",
              "    (embedding_layer): Linear(in_features=16, out_features=512, bias=True)\n",
              "    (encoder_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (multi_head_attention): MultiHeadAttentionLayer()\n",
              "        (feedforward): FeedforwardLayer(\n",
              "          (dense1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dense2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II) Test**"
      ],
      "metadata": {
        "id": "UilcepW4iCUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1) Récupération des données**"
      ],
      "metadata": {
        "id": "INCjtlQ1D10A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TvLOe9j8qHU",
        "outputId": "63e815f8-035f-4bc5-a962-64ed7111ef57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/IA/kaggle.json /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "TEY5ifr580Ri"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c digit-recognizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlxWJi7K86Us",
        "outputId": "1c5fe50a-686c-4431-cce6-6f45afbdbe05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "digit-recognizer.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip digit-recognizer.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiZBH8DN9FSb",
        "outputId": "35dba596-1c6c-45ab-fe69-1dfb691cc394"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  digit-recognizer.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2) Chargement des données et visualisation**"
      ],
      "metadata": {
        "id": "dZzyVxdRD7eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-3ODYsA99KvL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "print(df.shape)\n",
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "BQBUugpg9NW7",
        "outputId": "7acfadb0-487c-4213-8968-d167f2e35412"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 785)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
              "0      1       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel780  pixel781  pixel782  pixel783  \n",
              "0         0         0         0         0  \n",
              "\n",
              "[1 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab8d0957-370b-4068-898a-cfbe8e92a068\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab8d0957-370b-4068-898a-cfbe8e92a068')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab8d0957-370b-4068-898a-cfbe8e92a068 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab8d0957-370b-4068-898a-cfbe8e92a068');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['label'].to_numpy()\n",
        "X = df.drop(columns=['label']).to_numpy().reshape(-1, 28, 28)"
      ],
      "metadata": {
        "id": "fP2VwVHL9bNb"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[3])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mbW_OQC4EDjb",
        "outputId": "bd76cd23-2383-4659-ea99-2bea23e09a78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3) Dataloader**"
      ],
      "metadata": {
        "id": "J-NnRBSwijyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Data(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset qui contient deux listes de strings (text1 et text2) ainsi qu'une liste de similarités entre ces strings (target)\n",
        "    \"\"\"\n",
        "    def __init__(self, images, labels):      \n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return image, label\n",
        "  \n",
        "\n",
        "dataset = Data(X[:100], y[:100])\n",
        "test_size = 0.2\n",
        "nb_rows = len(dataset)\n",
        "nb_rows_train = int((1 - test_size) * nb_rows)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [nb_rows_train, nb_rows - nb_rows_train])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "3bay_L8oiSKl"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4) Define model and optimizer**"
      ],
      "metadata": {
        "id": "7Sww7-bKiKpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_length = 4\n",
        "nb_layers = 6\n",
        "embedding_dim = 512\n",
        "attention_heads = 8\n",
        "dropout_rate = 0.1\n",
        "num_classes = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "print(\"Number of patchs on an image : \", 28**2 / (patch_length**2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFyCJX8jYW_",
        "outputId": "cb26ef21-f28c-4625-df56-739a1c46a743"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patchs on an image :  49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(num_classes=num_classes, \n",
        "            nb_layers=nb_layers, \n",
        "            patch_length=patch_length, \n",
        "            embedding_dim=embedding_dim, \n",
        "            attention_heads=attention_heads, \n",
        "            dropout_rate=dropout_rate).to(device)\n",
        "loss_object = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "wCjap_kSjHEN"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5) Train**"
      ],
      "metadata": {
        "id": "71WoPUaCj-OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss_train = 0\n",
        "    running_loss_test = 0\n",
        "    print(\"Epoch {}/{}\".format(epoch+1, EPOCHS))\n",
        "    for images, labels in tqdm(train_dataloader):\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(images.to(device), training=True)\n",
        "        loss = loss_object(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss_train += loss.item()\n",
        "\n",
        "    running_loss_train /= len(train_dataloader)\n",
        "    losses_train.append(running_loss_train)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "          outputs = model(images.to(device))\n",
        "          loss = loss_object(outputs, labels.to(device))\n",
        "          running_loss_test += loss.item()\n",
        "\n",
        "    running_loss_test /= len(test_dataloader)\n",
        "    losses_test.append(running_loss_test)\n",
        "\n",
        "    print(\"Train Loss : {}, Test Loss : {}\".format(running_loss_train, running_loss_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "GZtzArq7j_7d",
        "outputId": "bdbfd9d6-eb29-4316-dbac-1a9b634dfc48"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.3283727963765464, Test Loss : 2.4223952293395996\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 2.343997319539388, Test Loss : 2.3499596118927\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-07017537551c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrunning_loss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}