{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64b9a72",
   "metadata": {
    "id": "b64b9a72"
   },
   "source": [
    "# **Reinforcement learning : Puissance 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3144711",
   "metadata": {
    "id": "f3144711"
   },
   "source": [
    "Le but de ce script est d'implémenter un agent capable de jouer au jeu Puissance 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86f4c5f9",
   "metadata": {
    "id": "86f4c5f9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc8bc3",
   "metadata": {
    "id": "e7cc8bc3"
   },
   "source": [
    "# 1) Création de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "qdop4Az4ouGJ",
   "metadata": {
    "id": "qdop4Az4ouGJ"
   },
   "outputs": [],
   "source": [
    "NB_ROWS = 6\n",
    "NB_COLUMNS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "294593f3",
   "metadata": {
    "id": "294593f3"
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"\n",
    "    Class that describes a game being played\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_rows=6, nb_columns=7, nb_players=2):\n",
    "        self.nb_rows = nb_rows\n",
    "        self.nb_columns = nb_columns\n",
    "        self.nb_players = nb_players\n",
    "        self.grid = np.zeros((nb_rows, nb_columns))\n",
    "        self.turn = 0\n",
    "        self.nb_steps = 0 # Total number of steps in the game\n",
    "        \n",
    "    def is_draw(self):\n",
    "        return self.nb_steps == self.nb_rows*self.nb_columns\n",
    "        \n",
    "    def is_legal(self, column):\n",
    "        # Takes as input a column and return whether it is possible or not to put a token or the column (True or False)\n",
    "        col_values = self.grid[:, column]\n",
    "        non_zeros, = np.where(col_values == 0)\n",
    "        return not (len(non_zeros) == 0)\n",
    "    \n",
    "    def set_token(self, column):\n",
    "        # Takes as input a column, and in the move is legal, put a token in the column\n",
    "        try:\n",
    "            assert self.is_legal(column)\n",
    "        except:\n",
    "            raise Exception(\"The move is not legal. Column {} is already filled\".format(column))\n",
    "        \n",
    "        row_index = np.where(self.grid[:, column] == 0)[0][0]\n",
    "        player_token = 1\n",
    "        if self.turn == 1:\n",
    "            player_token = -1\n",
    "            \n",
    "        self.grid[row_index][column] = player_token\n",
    "        \n",
    "        self.turn = (self.turn + 1) % self.nb_players\n",
    "        self.nb_steps += 1\n",
    "        \n",
    "    def won(self, player_id):\n",
    "        # Return True if the player n°player_id won, False otherwise\n",
    "        token_player = 1\n",
    "        if player_id == 1:\n",
    "            token_player = -1\n",
    "            \n",
    "        for row in range(self.nb_rows):\n",
    "            for column in range(self.nb_columns):\n",
    "                current_token = self.grid[row][column]\n",
    "                if current_token == token_player:\n",
    "                    token_array = token_player*np.ones(3)\n",
    "                    \n",
    "                    if np.array_equal(self.grid[row-3:row, column], token_array):\n",
    "                        return True\n",
    "                    \n",
    "                    elif np.array_equal(self.grid[row, column-3:column], token_array):\n",
    "                        return True\n",
    "                    \n",
    "                    elif np.array_equal(self.grid[row-3:row, column-3:column].diagonal(), token_array):\n",
    "                        return True\n",
    "                    \n",
    "        return False\n",
    "    \n",
    "    def get_observation(self):\n",
    "        # Returns the observation that the agent makes\n",
    "        return self.grid\n",
    "    \n",
    "    def render(self):\n",
    "        # Displays the current state of the game\n",
    "        X = []\n",
    "        Y = []\n",
    "        colors = []\n",
    "        color_player1 = np.array([210, 200, 0]) / 255\n",
    "        color_player2 = np.array([255, 0, 0]) / 255\n",
    "        for row in range(self.nb_rows):\n",
    "            for col in range(self.nb_columns):\n",
    "                token = self.grid[row, col]\n",
    "                if token == 1 or token == -1:\n",
    "                    Y.append(row)\n",
    "                    X.append(col)\n",
    "                    \n",
    "                    if token == 1:\n",
    "                        colors.append(color_player1)\n",
    "                    elif token == -1:\n",
    "                        colors.append(color_player2)\n",
    "                    \n",
    "        plt.scatter(X, Y, color=colors, s=600)\n",
    "        plt.xticks(list(range(-1, self.nb_columns+2)))\n",
    "        plt.yticks(list(range(-1, self.nb_rows+2)))\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f84d082b",
   "metadata": {
    "id": "f84d082b"
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \"\"\"\n",
    "    Class that represents the environment in which the agent is. \n",
    "    The agent is going to do multiple games in the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, actions=np.arange(NB_COLUMNS), nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_players=2):\n",
    "        self.nb_rows = nb_rows\n",
    "        self.nb_columns = nb_columns\n",
    "        self.nb_players = nb_players\n",
    "        self.game = Game(nb_rows=nb_rows, nb_columns=nb_columns, nb_players=nb_players)\n",
    "        self.winners = np.array([False, False])\n",
    "        self.actions = actions\n",
    "        self.nb_actions = len(actions)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Takes an action, which is the index of the column where we want to put a token in\n",
    "\n",
    "        # Tour de l'agent\n",
    "        observation = self.game.get_observation()\n",
    "        self.game.set_token(self.actions[action])\n",
    "        next_observation = self.game.get_observation()\n",
    "        reward = 0\n",
    "        game_finished = False\n",
    "        \n",
    "        if self.game.won(0):\n",
    "            self.winners[0] = True\n",
    "            reward = 1\n",
    "            game_finished = True\n",
    "            \n",
    "        elif self.game.is_draw():\n",
    "            reward = -1\n",
    "            game_finished = True\n",
    "\n",
    "        else:\n",
    "            # Tour du joueur 2\n",
    "            action_environment = np.random.choice([i for i in range(self.nb_actions) if self.game.is_legal(i)])\n",
    "            self.game.set_token(self.actions[action_environment])\n",
    "            next_observation = self.game.get_observation()\n",
    "\n",
    "            if self.game.won(1):\n",
    "                self.winners[1] = True\n",
    "                reward = -10\n",
    "                game_finished = True\n",
    "                \n",
    "            elif self.game.is_draw():\n",
    "                reward = -1\n",
    "                game_finished = True\n",
    "\n",
    "        return observation, reward, next_observation, game_finished\n",
    "\n",
    "\n",
    "    def is_game_finished(self):\n",
    "        return (True in self.winners)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game = Game(nb_rows=self.nb_rows, nb_columns=self.nb_columns, nb_players=self.nb_players)\n",
    "        self.winners = np.array([False, False])\n",
    "\n",
    "    def get_observation(self):\n",
    "      return self.game.get_observation()\n",
    "\n",
    "    def render(self):\n",
    "      self.game.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7985a",
   "metadata": {
    "id": "4ce7985a"
   },
   "source": [
    "# 2) Estimateur des valeurs d'actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6d71bbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6d71bbb",
    "outputId": "3bdc3ab6-60c7-4f9a-af48-4c95013074e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 7])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class Estimator(keras.Model):\n",
    "  def __init__(self, nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_actions=NB_COLUMNS):\n",
    "    super().__init__()\n",
    "    self.conv1 = layers.Conv2D(8, (4, 1), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "    self.conv2 = layers.Conv2D(8, (1, 4), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "    self.conv3 = layers.Conv2D(8, (4, 4), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "\n",
    "    self.conv4 = layers.Conv2D(8, (2, 2), padding='same', activation='relu')\n",
    "    self.conv5 = layers.Conv2D(8, (2, 2), activation='relu')\n",
    "    self.flatten = layers.Flatten()\n",
    "    self.pooling = layers.MaxPooling2D((2,2))\n",
    "    self.dense1 = layers.Dense(32, activation='relu')\n",
    "    self.dense2 = layers.Dense(nb_actions, activation='linear')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "\n",
    "    x1 = self.conv1(x)\n",
    "    x2 = self.conv2(x)\n",
    "    x3 = self.conv3(x)\n",
    "    \n",
    "    x = tf.concat([x1, x2, x3], axis=-1)\n",
    "    x = self.conv4(x)\n",
    "    \n",
    "    x = self.pooling(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    x = self.dense2(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "Q = Estimator()\n",
    "x = tf.random.uniform((32, NB_ROWS, NB_COLUMNS))\n",
    "Q(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpbKMlthmIdH",
   "metadata": {
    "id": "vpbKMlthmIdH"
   },
   "source": [
    "# 3) Définition des politiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ZI8x2VZmQbI",
   "metadata": {
    "id": "8ZI8x2VZmQbI"
   },
   "outputs": [],
   "source": [
    "class Control:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def __call__(self, action_value, observation, illegal_actions, type=\"greedy\"):\n",
    "        # type may be equal to greedy, epsilon_greedy or random\n",
    "        if type == 'greedy':\n",
    "            Q = action_value(observation).numpy()[0]\n",
    "            Q = self.set_impossible_actions_to_nan(Q, illegal_actions)\n",
    "            return np.nanargmax(Q)\n",
    "    \n",
    "        elif type == 'epsilon_greedy':\n",
    "            epsilon = 0.2\n",
    "            Q = action_value(observation).numpy()[0]\n",
    "            Q = self.set_impossible_actions_to_nan(Q, illegal_actions)\n",
    "\n",
    "            nb_legal_actions = (nb_actions - len(illegal_actions))\n",
    "\n",
    "            p = (epsilon/nb_legal_actions)*np.ones(nb_actions)\n",
    "            p[np.nanargmax(Q)] += (1 - epsilon)\n",
    "            for index in illegal_actions:\n",
    "                p[index] = 0\n",
    "\n",
    "            return np.random.choice([i for i in range(nb_actions)], p=p)\n",
    "        \n",
    "        elif type == 'random':\n",
    "            return np.random.choice([i for i in range(nb_actions) if i not in illegal_actions])\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Control {} is not implemented\".format(type))\n",
    "            \n",
    "    def set_impossible_actions_to_nan(self, action_values, illegal_actions):\n",
    "        # Takes an array of action_values for a given observation and a given action, and sets the impossible actions to np.nan, and returns the array\n",
    "        for index in illegal_actions:\n",
    "            action_values[index] = np.nan\n",
    "        \n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3aMvyLUnlKC",
   "metadata": {
    "id": "x3aMvyLUnlKC"
   },
   "source": [
    "# 4) Définition de la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dbalesS6nnWQ",
   "metadata": {
    "id": "dbalesS6nnWQ"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "  def __init__(self):\n",
    "    self.observations = []\n",
    "    self.actions = np.array([])\n",
    "    self.rewards = np.array([])\n",
    "    self.next_observations = []\n",
    "    self.dones = np.array([]) # True if game finished, False otherwise\n",
    "\n",
    "  def remember(self, obs, act, rew, next_obs, done):\n",
    "    self.observations.append(obs)\n",
    "    self.actions = np.append(self.actions, act).astype(np.float32)\n",
    "    self.rewards = np.append(self.rewards, rew).astype(np.float32)\n",
    "    self.next_observations.append(next_obs)\n",
    "    self.dones = np.append(self.dones, done).astype(np.float32)\n",
    "\n",
    "  def sample(self):\n",
    "    return np.array(self.observations), self.actions, self.rewards, np.array(self.next_observations), self.dones\n",
    "\n",
    "  def reset(self):\n",
    "    self.__init__()\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PsY9qf8qfY1e",
   "metadata": {
    "id": "PsY9qf8qfY1e"
   },
   "source": [
    "# 5) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "-p0L_Li5faOl",
   "metadata": {
    "id": "-p0L_Li5faOl"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, memory, action_value, optimizer, control):\n",
    "        self.actions = actions\n",
    "        self.nb_actions = len(actions)\n",
    "        self.memory = memory\n",
    "        self.action_value = action_value\n",
    "        self.optimizer = optimizer\n",
    "        self.control = control\n",
    "\n",
    "    def act(self, observation, illegal_actions, training=False):\n",
    "        global action\n",
    "        observation = tf.expand_dims(observation, axis=0)\n",
    "        if training:\n",
    "            action = self.control(self.action_value, observation, illegal_actions=illegal_actions, type='epsilon_greedy')\n",
    "        else:\n",
    "            action = self.control(self.action_value, observation, illegal_actions=illegal_actions, type='greedy')\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, gamma=0.99, batch_size=32):\n",
    "        agent_observations, agent_actions, agent_rewards, agent_next_observations, agent_dones = self.memory.sample()\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "          (agent_observations, agent_actions, agent_rewards, agent_next_observations, agent_dones)).shuffle(10000).batch(batch_size)\n",
    "        \n",
    "        for observations, actions, rewards, next_observations, dones in train_ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q = self.action_value(observations, training=True)\n",
    "\n",
    "                G_current = tf.convert_to_tensor([Q[i][int(act)] for i, act in enumerate(actions)])\n",
    "                max_Q_next_obs = np.argmax(self.action_value(next_observations, training=True), axis=1)\n",
    "\n",
    "                G_next = tf.cast((1-dones)*max_Q_next_obs, tf.float32)\n",
    "\n",
    "                loss = tf.math.reduce_sum((G_current - (rewards + G_next))**2)\n",
    "                gradients = tape.gradient(loss, self.action_value.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.action_value.trainable_variables))\n",
    "          \n",
    "        self.reset()\n",
    "\n",
    "    def remember(self, obs, act, rew, next_obs, done):\n",
    "        self.memory.remember(obs, act, rew, next_obs, done)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TJvqH5pCoeWR",
   "metadata": {
    "id": "TJvqH5pCoeWR"
   },
   "source": [
    "# 6) Boucle de jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "h_0zTBb5xZqo",
   "metadata": {
    "id": "h_0zTBb5xZqo"
   },
   "outputs": [],
   "source": [
    "actions = np.arange(NB_COLUMNS)\n",
    "nb_actions = len(actions)\n",
    "\n",
    "env = Env(actions=actions, nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_players=2)\n",
    "action_value = Estimator(nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_actions=nb_actions)\n",
    "memory = Memory()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "control = Control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8r8YNVUggfXO",
   "metadata": {
    "id": "8r8YNVUggfXO"
   },
   "outputs": [],
   "source": [
    "agent = Agent(actions, memory, action_value, optimizer, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8VP5s1YQogIA",
   "metadata": {
    "id": "8VP5s1YQogIA"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "nb_games_before_update = 20\n",
    "nb_games_test = 10\n",
    "nb_wins_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"--- Epoch {}/{} ---\".format(epoch+1, EPOCHS))\n",
    "\n",
    "    # Train\n",
    "    for _ in range(nb_games_before_update):\n",
    "        game_finished = False\n",
    "        while not game_finished:\n",
    "            observation = env.get_observation()\n",
    "            illegal_actions = [i for i in range(nb_actions) if not env.game.is_legal(i)]\n",
    "\n",
    "            action = agent.act(observation, illegal_actions, training=True)\n",
    "            observation, reward, next_observation, game_finished = env.step(action)            \n",
    "            agent.remember(observation, action, reward, next_observation, game_finished)\n",
    "        \n",
    "        env.reset()\n",
    "    agent.learn()\n",
    "    \n",
    "    # Test\n",
    "    nb_wins = 0\n",
    "    for _ in range(nb_games_test):\n",
    "        game_finished = False\n",
    "        while not game_finished:\n",
    "            illegal_actions = [i for i in range(nb_actions) if not env.game.is_legal(i)]\n",
    "            action = agent.act(env.get_observation(), illegal_actions, training=False)\n",
    "\n",
    "            _, _, _, game_finished = env.step(action)\n",
    "        \n",
    "        if env.game.won(0):\n",
    "            nb_wins += 1\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    nb_wins_history.append(nb_wins)\n",
    "    print(\"{}/{} wins\".format(nb_wins, nb_games_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gAvg5WhYE1On",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "gAvg5WhYE1On",
    "outputId": "d424f9c8-2bee-4b45-da3e-597b0367e0dc"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(EPOCHS), nb_wins_history)\n",
    "plt.title(\"Evolution du nombre de victoires de l'agent\")\n",
    "plt.xticks(np.arange(EPOCHS))\n",
    "plt.yticks([i for i in range(nb_games_test+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Nombre de victoires\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQr6GTksoC1f",
   "metadata": {
    "id": "EQr6GTksoC1f"
   },
   "source": [
    "# 7) Test de partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ECq4RmunRF-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ECq4RmunRF-",
    "outputId": "9ff9e3cc-d73b-4247-dcb9-876b4bf94fcc"
   },
   "outputs": [],
   "source": [
    "game = Game()\n",
    "game_finished = False\n",
    "while not game_finished:\n",
    "    illegal_actions = [i for i in range(nb_actions) if not game.is_legal(i)]\n",
    "    action = agent.act(game.get_observation(), illegal_actions, training=True)\n",
    "    game.set_token(action)\n",
    "    game.render()\n",
    "\n",
    "    if game.won(0):\n",
    "        game_finished = True\n",
    "    \n",
    "    elif game.is_draw():\n",
    "        game_finished = True\n",
    "    \n",
    "    else:\n",
    "        player_action = int(input((\"Choisissez une colonne : \")))\n",
    "        game.set_token(player_action)\n",
    "        if game.won(1) or game.is_draw():\n",
    "            game_finished = True"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
