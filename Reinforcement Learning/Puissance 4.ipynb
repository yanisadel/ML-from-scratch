{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64b9a72",
   "metadata": {
    "id": "b64b9a72"
   },
   "source": [
    "# **Reinforcement learning : Puissance 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3144711",
   "metadata": {
    "id": "f3144711"
   },
   "source": [
    "Le but de ce script est d'implémenter un agent capable de jouer au jeu Puissance 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f4c5f9",
   "metadata": {
    "id": "86f4c5f9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc8bc3",
   "metadata": {
    "id": "e7cc8bc3"
   },
   "source": [
    "# 1) Création de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qdop4Az4ouGJ",
   "metadata": {
    "id": "qdop4Az4ouGJ"
   },
   "outputs": [],
   "source": [
    "NB_ROWS = 6\n",
    "NB_COLUMNS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294593f3",
   "metadata": {
    "id": "294593f3"
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"\n",
    "    Class that describes a game being played\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_rows=6, nb_columns=7, nb_players=2):\n",
    "        self.nb_rows = nb_rows\n",
    "        self.nb_columns = nb_columns\n",
    "        self.nb_players = nb_players\n",
    "        self.grid = np.zeros((nb_rows, nb_columns))\n",
    "        self.turn = 0\n",
    "        self.nb_steps = 0 # Total number of steps in the game\n",
    "        \n",
    "    def is_draw(self):\n",
    "        return self.nb_steps == self.nb_rows*self.nb_columns\n",
    "        \n",
    "    def is_legal(self, column):\n",
    "        # Takes as input a column and return whether it is possible or not to put a token or the column (True or False)\n",
    "        col_values = self.grid[:, column]\n",
    "        non_zeros, = np.where(col_values == 0)\n",
    "        return not (len(non_zeros) == 0)\n",
    "    \n",
    "    def set_token(self, column):\n",
    "        # Takes as input a column, and in the move is legal, put a token in the column\n",
    "        try:\n",
    "            assert self.is_legal(column)\n",
    "        except:\n",
    "            raise Exception(\"The move is not legal. Column {} is already filled\".format(column))\n",
    "        \n",
    "        row_index = np.where(self.grid[:, column] == 0)[0][0]\n",
    "        player_token = 1\n",
    "        if self.turn == 1:\n",
    "            player_token = -1\n",
    "            \n",
    "        self.grid[row_index][column] = player_token\n",
    "        \n",
    "        self.turn = (self.turn + 1) % self.nb_players\n",
    "        self.nb_steps += 1\n",
    "        \n",
    "    def won(self, player_id):\n",
    "        # Return True if the player n°player_id won, False otherwise\n",
    "        token_player = 1\n",
    "        if player_id == 1:\n",
    "            token_player = -1\n",
    "            \n",
    "        for row in range(self.nb_rows):\n",
    "            for column in range(self.nb_columns):\n",
    "                current_token = self.grid[row][column]\n",
    "                if current_token == token_player:\n",
    "                    token_array = token_player*np.ones(3)\n",
    "                    \n",
    "                    if np.array_equal(self.grid[row-3:row, column], token_array):\n",
    "                        return True\n",
    "                    \n",
    "                    elif np.array_equal(self.grid[row, column-3:column], token_array):\n",
    "                        return True\n",
    "                    \n",
    "                    elif np.array_equal(self.grid[row-3:row, column-3:column].diagonal(), token_array):\n",
    "                        return True\n",
    "                    \n",
    "        return False\n",
    "    \n",
    "    def get_observation(self):\n",
    "        # Returns the observation that the agent makes\n",
    "        return self.grid\n",
    "    \n",
    "    def render(self):\n",
    "        # Displays the current state of the game\n",
    "        X = []\n",
    "        Y = []\n",
    "        colors = []\n",
    "        color_player1 = np.array([210, 200, 0]) / 255\n",
    "        color_player2 = np.array([255, 0, 0]) / 255\n",
    "        for row in range(self.nb_rows):\n",
    "            for col in range(self.nb_columns):\n",
    "                token = self.grid[row, col]\n",
    "                if token == 1 or token == -1:\n",
    "                    Y.append(row)\n",
    "                    X.append(col)\n",
    "                    \n",
    "                    if token == 1:\n",
    "                        colors.append(color_player1)\n",
    "                    elif token == -1:\n",
    "                        colors.append(color_player2)\n",
    "                    \n",
    "        plt.scatter(X, Y, color=colors, s=600)\n",
    "        plt.xticks(list(range(-1, self.nb_columns+2)))\n",
    "        plt.yticks(list(range(-1, self.nb_rows+2)))\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84d082b",
   "metadata": {
    "id": "f84d082b"
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \"\"\"\n",
    "    Class that represents the environment in which the agent is. \n",
    "    The agent is going to do multiple games in the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, actions=np.arange(NB_COLUMNS), nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_players=2):\n",
    "        self.nb_rows = nb_rows\n",
    "        self.nb_columns = nb_columns\n",
    "        self.nb_players = nb_players\n",
    "        self.game = Game(nb_rows=nb_rows, nb_columns=nb_columns, nb_players=nb_players)\n",
    "        self.winners = np.array([False, False])\n",
    "        self.actions = actions\n",
    "        self.nb_actions = len(actions)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Takes an action, which is the index of the column where we want to put a token in\n",
    "\n",
    "        # Tour de l'agent\n",
    "        observation = self.game.get_observation()\n",
    "        self.game.set_token(self.actions[action])\n",
    "        next_observation = self.game.get_observation()\n",
    "        reward = 0\n",
    "        game_finished = False\n",
    "        \n",
    "        if self.game.won(0):\n",
    "            self.winners[0] = True\n",
    "            reward = 1\n",
    "            game_finished = True\n",
    "            \n",
    "        elif self.game.is_draw():\n",
    "            reward = -1\n",
    "            game_finished = True\n",
    "\n",
    "        else:\n",
    "            # Tour du joueur 2\n",
    "            action_environment = np.random.choice([i for i in range(self.nb_actions) if self.game.is_legal(i)])\n",
    "            self.game.set_token(self.actions[action_environment])\n",
    "            next_observation = self.game.get_observation()\n",
    "\n",
    "            if self.game.won(1):\n",
    "                self.winners[1] = True\n",
    "                reward = -10\n",
    "                game_finished = True\n",
    "                \n",
    "            elif self.game.is_draw():\n",
    "                reward = -1\n",
    "                game_finished = True\n",
    "\n",
    "        return observation, reward, next_observation, game_finished\n",
    "\n",
    "\n",
    "    def is_game_finished(self):\n",
    "        return (True in self.winners)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game = Game(nb_rows=self.nb_rows, nb_columns=self.nb_columns, nb_players=self.nb_players)\n",
    "        self.winners = np.array([False, False])\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.game.get_observation()\n",
    "\n",
    "    def render(self):\n",
    "        self.game.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7985a",
   "metadata": {
    "id": "4ce7985a"
   },
   "source": [
    "# 2) Estimateur des valeurs d'actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d71bbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6d71bbb",
    "outputId": "3bdc3ab6-60c7-4f9a-af48-4c95013074e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class Estimator(keras.Model):\n",
    "    def __init__(self, nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_actions=NB_COLUMNS):\n",
    "        super().__init__()\n",
    "        self.conv1 = layers.Conv2D(8, (4, 1), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "        self.conv2 = layers.Conv2D(8, (1, 4), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "        self.conv3 = layers.Conv2D(8, (4, 4), padding='same', activation='relu', input_shape=(nb_rows, nb_columns, 1))\n",
    "\n",
    "        self.conv4 = layers.Conv2D(8, (2, 2), padding='same', activation='relu')\n",
    "        self.conv5 = layers.Conv2D(8, (2, 2), activation='relu')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.pooling = layers.MaxPooling2D((2,2))\n",
    "        self.dense1 = layers.Dense(32, activation='relu')\n",
    "        self.dense2 = layers.Dense(nb_actions, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.expand_dims(x, axis=-1)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "\n",
    "        x = tf.concat([x1, x2, x3], axis=-1)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "Q = Estimator()\n",
    "x = tf.random.uniform((32, NB_ROWS, NB_COLUMNS))\n",
    "Q(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vpbKMlthmIdH",
   "metadata": {
    "id": "vpbKMlthmIdH"
   },
   "source": [
    "# 3) Définition des politiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ZI8x2VZmQbI",
   "metadata": {
    "id": "8ZI8x2VZmQbI"
   },
   "outputs": [],
   "source": [
    "class Control:\n",
    "    def __init__(self, nb_actions):\n",
    "        self.nb_actions = nb_actions\n",
    "  \n",
    "    def __call__(self, action_value, observation, illegal_actions, type=\"greedy\"):\n",
    "        # type may be equal to greedy, epsilon_greedy or random\n",
    "        if type == 'greedy':\n",
    "            Q = action_value(observation).numpy()[0]\n",
    "            Q = self.set_impossible_actions_to_nan(Q, illegal_actions)\n",
    "            return np.nanargmax(Q)\n",
    "    \n",
    "        elif type == 'epsilon_greedy':\n",
    "            epsilon = 0.2\n",
    "            Q = action_value(observation).numpy()[0]\n",
    "            Q = self.set_impossible_actions_to_nan(Q, illegal_actions)\n",
    "\n",
    "            nb_legal_actions = (self.nb_actions - len(illegal_actions))\n",
    "\n",
    "            p = (epsilon/nb_legal_actions)*np.ones(self.nb_actions)\n",
    "            p[np.nanargmax(Q)] += (1 - epsilon)\n",
    "            for index in illegal_actions:\n",
    "                p[index] = 0\n",
    "\n",
    "            return np.random.choice([i for i in range(self.nb_actions)], p=p)\n",
    "        \n",
    "        elif type == 'random':\n",
    "            return np.random.choice([i for i in range(self.nb_actions) if i not in illegal_actions])\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Control {} is not implemented\".format(type))\n",
    "            \n",
    "    def set_impossible_actions_to_nan(self, action_values, illegal_actions):\n",
    "        # Takes an array of action_values for a given observation and a given action, and sets the impossible actions to np.nan, and returns the array\n",
    "        for index in illegal_actions:\n",
    "            action_values[index] = np.nan\n",
    "        \n",
    "        return action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3aMvyLUnlKC",
   "metadata": {
    "id": "x3aMvyLUnlKC"
   },
   "source": [
    "# 4) Définition de la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbalesS6nnWQ",
   "metadata": {
    "id": "dbalesS6nnWQ"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.observations = []\n",
    "        self.actions = np.array([])\n",
    "        self.rewards = np.array([])\n",
    "        self.next_observations = []\n",
    "        self.dones = np.array([]) # True if game finished, False otherwise\n",
    "\n",
    "    def remember(self, obs, act, rew, next_obs, done):\n",
    "        self.observations.append(obs)\n",
    "        self.actions = np.append(self.actions, act).astype(np.float32)\n",
    "        self.rewards = np.append(self.rewards, rew).astype(np.float32)\n",
    "        self.next_observations.append(next_obs)\n",
    "        self.dones = np.append(self.dones, done).astype(np.float32)\n",
    "\n",
    "    def sample(self):\n",
    "        return np.array(self.observations), self.actions, self.rewards, np.array(self.next_observations), self.dones\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfe028",
   "metadata": {},
   "source": [
    "# 5) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fccd2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, action_value, observations, actions, rewards, next_observations, dones):\n",
    "        Q = action_value(observations, training=True)\n",
    "        G_current = tf.convert_to_tensor([Q[i][int(act)] for i, act in enumerate(actions)])\n",
    "        max_Q_next_obs = np.argmax(action_value(next_observations, training=True), axis=1)\n",
    "\n",
    "        G_next = tf.cast((1-dones)*max_Q_next_obs, tf.float32)\n",
    "\n",
    "        loss = tf.math.reduce_sum((G_current - (rewards + G_next))**2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PsY9qf8qfY1e",
   "metadata": {
    "id": "PsY9qf8qfY1e"
   },
   "source": [
    "# 6) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "-p0L_Li5faOl",
   "metadata": {
    "id": "-p0L_Li5faOl"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, memory, action_value, optimizer, control, evaluation):\n",
    "        self.actions = actions\n",
    "        self.nb_actions = len(actions)\n",
    "        self.memory = memory\n",
    "        self.action_value = action_value\n",
    "        self.optimizer = optimizer\n",
    "        self.control = control\n",
    "        self.evaluation = evaluation\n",
    "\n",
    "    def act(self, observation, illegal_actions, training=False):\n",
    "        global action\n",
    "        observation = tf.expand_dims(observation, axis=0)\n",
    "        if training:\n",
    "            action = self.control(self.action_value, observation, illegal_actions=illegal_actions, type='epsilon_greedy')\n",
    "        else:\n",
    "            action = self.control(self.action_value, observation, illegal_actions=illegal_actions, type='greedy')\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, batch_size=32):\n",
    "        agent_observations, agent_actions, agent_rewards, agent_next_observations, agent_dones = self.memory.sample()\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "          (agent_observations, agent_actions, agent_rewards, agent_next_observations, agent_dones)).shuffle(10000).batch(batch_size)\n",
    "        \n",
    "        for observations, actions, rewards, next_observations, dones in train_ds:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = self.evaluation(self.action_value, observations, actions, rewards, next_observations, dones)\n",
    "                gradients = tape.gradient(loss, self.action_value.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.action_value.trainable_variables))\n",
    "          \n",
    "        self.reset()\n",
    "\n",
    "    def remember(self, obs, act, rew, next_obs, done):\n",
    "        self.memory.remember(obs, act, rew, next_obs, done)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TJvqH5pCoeWR",
   "metadata": {
    "id": "TJvqH5pCoeWR"
   },
   "source": [
    "# 7) Boucle de jeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "h_0zTBb5xZqo",
   "metadata": {
    "id": "h_0zTBb5xZqo"
   },
   "outputs": [],
   "source": [
    "actions = np.arange(NB_COLUMNS)\n",
    "nb_actions = len(actions)\n",
    "env = Env(actions=actions, nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_players=2)\n",
    "\n",
    "control = Control(nb_actions)\n",
    "evaluation = Evaluation()\n",
    "memory = Memory()\n",
    "\n",
    "action_value = Estimator(nb_rows=NB_ROWS, nb_columns=NB_COLUMNS, nb_actions=nb_actions)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "agent = Agent(actions, memory, action_value, optimizer, control, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8VP5s1YQogIA",
   "metadata": {
    "id": "8VP5s1YQogIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/50 ---\n",
      "9/10 wins\n",
      "--- Epoch 2/50 ---\n",
      "9/10 wins\n",
      "--- Epoch 3/50 ---\n",
      "9/10 wins\n",
      "--- Epoch 4/50 ---\n",
      "8/10 wins\n",
      "--- Epoch 5/50 ---\n",
      "9/10 wins\n",
      "--- Epoch 6/50 ---\n",
      "8/10 wins\n",
      "--- Epoch 7/50 ---\n",
      "9/10 wins\n",
      "--- Epoch 8/50 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game_finished:\n\u001b[0;32m     28\u001b[0m     illegal_actions \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_actions) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mis_legal(i)]\n\u001b[1;32m---> 29\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43millegal_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     _, _, _, game_finished \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mwon(\u001b[38;5;241m0\u001b[39m):\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self, observation, illegal_actions, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_value, observation, illegal_actions\u001b[38;5;241m=\u001b[39millegal_actions, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_greedy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43millegal_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43millegal_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgreedy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mControl.__call__\u001b[1;34m(self, action_value, observation, illegal_actions, type)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, action_value, observation, illegal_actions, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# type may be equal to greedy, epsilon_greedy or random\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m         Q \u001b[38;5;241m=\u001b[39m \u001b[43maction_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m         Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_impossible_actions_to_nan(Q, illegal_actions)\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnanargmax(Q)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1096\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mEstimator.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)\n\u001b[1;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1(x)\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1096\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1096\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\core\\flatten.py:79\u001b[0m, in \u001b[0;36mFlatten.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m     76\u001b[0m   \u001b[38;5;66;03m# Full static shape is guaranteed to be available.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m   \u001b[38;5;66;03m# Performance: Using `constant_op` is much faster than passing a list.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m   flattened_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m   input_shape \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:194\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanip.reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     59\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(tensor, shape, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m   tensor_util\u001b[38;5;241m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8532\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   8531\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 8532\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8533\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   8535\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "nb_games_before_update = 20\n",
    "nb_games_test = 10\n",
    "nb_wins_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"--- Epoch {}/{} ---\".format(epoch+1, EPOCHS))\n",
    "\n",
    "    # Train\n",
    "    for _ in range(nb_games_before_update):\n",
    "        game_finished = False\n",
    "        while not game_finished:\n",
    "            observation = env.get_observation()\n",
    "            illegal_actions = [i for i in range(nb_actions) if not env.game.is_legal(i)]\n",
    "\n",
    "            action = agent.act(observation, illegal_actions, training=True)\n",
    "            observation, reward, next_observation, game_finished = env.step(action)            \n",
    "            agent.remember(observation, action, reward, next_observation, game_finished)\n",
    "        \n",
    "        env.reset()\n",
    "    agent.learn()\n",
    "    \n",
    "    # Test\n",
    "    nb_wins = 0\n",
    "    for _ in range(nb_games_test):\n",
    "        game_finished = False\n",
    "        while not game_finished:\n",
    "            illegal_actions = [i for i in range(nb_actions) if not env.game.is_legal(i)]\n",
    "            action = agent.act(env.get_observation(), illegal_actions, training=False)\n",
    "\n",
    "            _, _, _, game_finished = env.step(action)\n",
    "        \n",
    "        if env.game.won(0):\n",
    "            nb_wins += 1\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    nb_wins_history.append(nb_wins)\n",
    "    print(\"{}/{} wins\".format(nb_wins, nb_games_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gAvg5WhYE1On",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "gAvg5WhYE1On",
    "outputId": "d424f9c8-2bee-4b45-da3e-597b0367e0dc"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(EPOCHS), nb_wins_history)\n",
    "plt.title(\"Evolution du nombre de victoires de l'agent\")\n",
    "plt.xticks(np.arange(EPOCHS))\n",
    "plt.yticks([i for i in range(nb_games_test+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Nombre de victoires\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQr6GTksoC1f",
   "metadata": {
    "id": "EQr6GTksoC1f"
   },
   "source": [
    "# 8) Test de partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ECq4RmunRF-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ECq4RmunRF-",
    "outputId": "9ff9e3cc-d73b-4247-dcb9-876b4bf94fcc"
   },
   "outputs": [],
   "source": [
    "game = Game()\n",
    "game_finished = False\n",
    "while not game_finished:\n",
    "    illegal_actions = [i for i in range(nb_actions) if not game.is_legal(i)]\n",
    "    action = agent.act(game.get_observation(), illegal_actions, training=True)\n",
    "    game.set_token(action)\n",
    "    game.render()\n",
    "\n",
    "    if game.won(0):\n",
    "        game_finished = True\n",
    "    \n",
    "    elif game.is_draw():\n",
    "        game_finished = True\n",
    "    \n",
    "    else:\n",
    "        player_action = int(input((\"Choisissez une colonne : \")))\n",
    "        game.set_token(player_action)\n",
    "        if game.won(1) or game.is_draw():\n",
    "            game_finished = True"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
